{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "''' TO DO LIST\n",
    "err_list 파일 자동화\n",
    "col_list\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import yeojohnson\n",
    "from tensorflow.python.keras.optimizer_v2.rmsprop import RMSProp\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, RepeatVector, LSTM, Input, TimeDistributed, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.compose import ColumnTransformer\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#variables\n",
    "powhr_start = 5\n",
    "powhr_end   = 20\n",
    "shift_days  = 3\n",
    "hoursteps   = powhr_end-powhr_start+1 #(16)\n",
    "timesteps   = shift_days*hoursteps #hours step\n",
    "data_dim    = 7\n",
    "out_dim     = 1\n",
    "n_model     = 10\n",
    "\n",
    "#data_dir   = '../Data'\n",
    "data_dir  = 'C:/Users/VISLAB_PHY/Desktop/Workspace/Data'\n",
    "season_mod = 'all_1102_f7'\n",
    "date_start = '10190901'\n",
    "date_end   = '30191201'\n",
    "err_date_list = ['20190912', '20191122', '20191130', '20191028', '20191107', '20191108', '20191109', '20191110', '20191111', '20191112', '20200214', '20200307', '20200308', '20200309', '20200310', '20200328', '20200329', '20200625', '20200809']\n",
    "err_date_list = ['20190912',\n",
    "                '20191122',\n",
    "                '20191130',\n",
    "                '20191217',\n",
    "                '20200501',\n",
    "                '20200502',\n",
    "                '20191028',\n",
    "                '20191107',\n",
    "                '20191108',\n",
    "                '20191109',\n",
    "                '20191110',\n",
    "                '20191111',\n",
    "                '20191112',\n",
    "                '20200214',\n",
    "                '20200307',\n",
    "                '20200308',\n",
    "                '20200309',\n",
    "                '20200310',\n",
    "                '20200328',\n",
    "                '20200329',\n",
    "                '20200625',\n",
    "                '20200809']\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 종관기상관측\n",
    "#############################################\n",
    "def get_weather():\n",
    "    # pow 파일 load\n",
    "    file_list   = os.listdir(data_dir)\n",
    "    print(len(file_list))\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if fnmatch.fnmatch(filename, 'OBS_ASOS_TIM_*.csv'):\n",
    "            print(filename)\n",
    "\n",
    "            # load csv data\n",
    "            dataset = read_csv(data_dir+'/'+filename, encoding='CP949')\n",
    "            dataset.drop(['지점','지점명'], axis=1, inplace=True)\n",
    "            dataset.drop(['기온 QC플래그','강수량 QC플래그','풍속 QC플래그','풍향 QC플래그','습도 QC플래그'], axis=1, inplace=True)\n",
    "            dataset.drop(['현지기압 QC플래그','해면기압 QC플래그','일조 QC플래그','지면온도 QC플래그'], axis=1, inplace=True)\n",
    "            dataset.drop(['5cm 지중온도(°C)','10cm 지중온도(°C)','20cm 지중온도(°C)','30cm 지중온도(°C)'], axis=1, inplace=True)\n",
    "            dataset.drop(['3시간신적설(cm)','일사(MJ/m2)','운형(운형약어)','지면상태(지면상태코드)','현상번호(국내식)'], axis=1, inplace=True)\n",
    "\n",
    "            # set column name\n",
    "            dataset.columns = ['ymdhms', 'temprt', 'rain', 'wnd_spd', 'wnd_dir', 'humdt','steampressr', 'dewpnt', 'pressr','seapressr','sunshine','snow','cloud','cloud2','mincloud','visiblt','grd_temprt']\n",
    "\n",
    "            # prioirty sort (피어슨상관계수)\n",
    "            dataset = dataset[['ymdhms','sunshine','humdt','wnd_spd','visiblt','cloud2', 'cloud','grd_temprt','wnd_dir','dewpnt','steampressr','temprt','mincloud','rain','pressr','seapressr','snow']]\n",
    "\n",
    "            # set NA data (관측값 0이 누적되어 결측된 경우. 0으로 세팅)\n",
    "            dataset['rain'].fillna(0, inplace=True)     #강수량\n",
    "            dataset['sunshine'].fillna(0, inplace=True) #일조\n",
    "            dataset['snow'].fillna(0, inplace=True)     #적설량\n",
    "\n",
    "            #일시 패턴 변환(2019-08-20 5:00 -> 2019082005)\n",
    "            dataset['ymdhms'] = dataset['ymdhms'].str[0:4]+dataset['ymdhms'].str[5:7]+dataset['ymdhms'].str[8:10]+dataset['ymdhms'].str[11:13]\n",
    "            # pow측정값 중 결측값 많은 일자 제거\n",
    "            dataset = dataset[(dataset['ymdhms'].str[0:8]>=date_start) & (dataset['ymdhms'].str[0:8]<date_end)]\n",
    "            for err_date in err_date_list:\n",
    "                idx_err = dataset[dataset['ymdhms'].str.startswith(err_date)].index\n",
    "                dataset = dataset.drop(idx_err)\n",
    "\n",
    "            #낮시간 추출 (5~20시)\n",
    "            dataset = dataset[(dataset['ymdhms'].str[-2:]>=str(powhr_start).rjust(2, '0')) &(dataset['ymdhms'].str[-2:]<=str(powhr_end))]\n",
    "            dataset = dataset.interpolate(method='linear')# 결측값 보간\n",
    "            \n",
    "            # save file (test용)\n",
    "            dataset.to_csv(data_dir+\"/weather.csv\",mode='w',index=False)\n",
    "\n",
    "            # normalization\n",
    "            dataset.drop(['ymdhms'], axis=1, inplace=True)\n",
    "            dataset = dataset.astype('float32')\n",
    "            dataset = dataset.interpolate(method='linear')\n",
    "            \n",
    "            #YEO-JOHNSON transform\n",
    "            yeo_df = yeo_johnson_transform(dataset)\n",
    "        \n",
    "            sc = MinMaxScaler(feature_range = (0, 1))#scale\n",
    "            scaled_weather = sc.fit_transform(yeo_df.values)\n",
    "            weather = pd.DataFrame(scaled_weather, columns=yeo_df.columns, index=list(yeo_df.index.values))\n",
    "            print(\"before : \", weather.shape)\n",
    "            weather = weather.iloc[:, 0:data_dim] #feature size 조절\n",
    "            print(\"after : \", weather.shape)\n",
    "            '''\n",
    "            모든 컬럼 돌림. JSON 변환해야해\n",
    "            '''\n",
    "            print(\"type(weather) : \", type(weather))\n",
    "            test = {}\n",
    "            print(\"********** test _df\")\n",
    "            for col in weather.columns:\n",
    "                test[col] = list(np.histogram(weather[col], bins=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]))    \n",
    "    return test\n",
    "\n",
    "#############################################\n",
    "# 태양광 전력\n",
    "#############################################\n",
    "def get_pow():\n",
    "\n",
    "    # pow 파일 load\n",
    "    dir_path    = data_dir+\"/pow_24/UR00000126_csv\"\n",
    "    file_list   = os.listdir(dir_path)\n",
    "    print(len(file_list))\n",
    "    hrPow  = []    \n",
    "\n",
    "    # pow측정값 에러가 큰 일자 제거\n",
    "    for filename in file_list:\n",
    "        if (filename[:-4] not in err_date_list):\n",
    "            if ((filename[:-4]>=date_start) & (filename<date_end)):\n",
    "                filedata = pd.read_csv(dir_path+'/'+filename).values[:,0]\n",
    "                hrPow.append(filedata)\n",
    "                \n",
    "    #낮시간 추출 (5~20시)\n",
    "    pow_dataset = pd.DataFrame(hrPow)\n",
    "    pow_dataset =pow_dataset.iloc[:,powhr_start:powhr_end+1]\n",
    "    #pow_dataset.to_csv(\"C:/Users/VISLAB_PHY/Desktop/WORKSPACE/Origin/data/pow_hr.csv\",mode='w',index=False)\n",
    "\n",
    "    # 결측값 보간, reshape\n",
    "    pow_dataset = pow_dataset.interpolate(method='linear')\n",
    "    pow_dataset = pow_dataset.values.reshape(-1,1)\n",
    "    pow_dataset = pd.DataFrame(pow_dataset)\n",
    "    pow_dataset.columns = ['pow']\n",
    "    pow_dataset.to_csv(data_dir+\"/pow.csv\",mode='w',index=False)\n",
    "    \n",
    "    # scale\n",
    "    sc_pow = MinMaxScaler(feature_range = (0, 1))\n",
    "    scaled_pow = sc_pow.fit_transform(pow_dataset.values)\n",
    "    df_pow = pd.DataFrame(scaled_pow, columns=pow_dataset.columns, index=list(pow_dataset.index.values))\n",
    "    \n",
    "    return df_pow, sc_pow\n",
    "\n",
    "#############################################\n",
    "# Yeo-Johnson Transformation\n",
    "#############################################\n",
    "transf_type = 'yeo-johnson'\n",
    "#transf_type = 'box-cox'\n",
    "\n",
    "def yeo_johnson_transform(dataset):\n",
    "    \n",
    "    column_trans = ColumnTransformer(\n",
    "                    [\n",
    "                        ('sunshine', PowerTransformer(method=transf_type, standardize=True), ['sunshine']),\n",
    "                        ('humdt', PowerTransformer(method=transf_type, standardize=True), ['humdt']),\n",
    "                        ('wnd_spd', PowerTransformer(method=transf_type, standardize=True), ['wnd_spd']),\n",
    "                        ('visiblt', PowerTransformer(method=transf_type, standardize=True), ['visiblt']),\n",
    "                        ('cloud2', PowerTransformer(method=transf_type, standardize=True), ['cloud2']),\n",
    "                        ('cloud', PowerTransformer(method=transf_type, standardize=True), ['cloud']),\n",
    "                        ('grd_temprt', PowerTransformer(method=transf_type, standardize=True), ['grd_temprt']),\n",
    "                        ('wnd_dir', PowerTransformer(method=transf_type, standardize=True), ['wnd_dir']),\n",
    "                        ('dewpnt', PowerTransformer(method=transf_type, standardize=True), ['dewpnt']),\n",
    "                        ('steampressr', PowerTransformer(method=transf_type, standardize=True), ['steampressr']),\n",
    "                        ('temprt', PowerTransformer(method=transf_type, standardize=True), ['temprt']),\n",
    "                        ('mincloud', PowerTransformer(method=transf_type, standardize=True), ['mincloud']),\n",
    "                        ('rain', PowerTransformer(method=transf_type, standardize=True), ['rain']),\n",
    "                        ('pressr', PowerTransformer(method=transf_type, standardize=True), ['pressr']),\n",
    "                        ('seapressr', PowerTransformer(method=transf_type, standardize=True), ['seapressr']),\n",
    "                        ('snow', PowerTransformer(method=transf_type, standardize=True), ['snow'])\n",
    "                    ])\n",
    "    \n",
    "    transformed_data = column_trans.fit_transform(dataset)\n",
    "    transformed_df = pd.DataFrame(transformed_data, columns=dataset.columns)\n",
    "    pd.concat([transformed_df], axis = 1)\n",
    "                    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def temp():\n",
    "    #############################################\n",
    "    # numpy data 만들기\n",
    "    #############################################\n",
    "    df_pow, sc_pow   = get_pow()\n",
    "    df               = get_weather()\n",
    "\n",
    "    # pow + weather + powY\n",
    "    df.insert(0, 'pow', df_pow.values, True)\n",
    "    df = df.iloc[0:-timesteps, :]\n",
    "    df.insert(df.shape[1], 'pow_Y', df_pow.iloc[timesteps:, :].values, True)\n",
    "    df.to_csv(data_dir+\"/total.csv\",mode='w',index=False, encoding='CP949')\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # time step만큼 window 움직여 dataset 생성\n",
    "    #----------------------------------------------\n",
    "    totalsize = df.shape[0]\n",
    "    dataX, dataY = [], []\n",
    "\n",
    "    for i in range(0, totalsize-timesteps-24+1, hoursteps):\n",
    "        dataX.append(df.iloc[i:(i + timesteps),0:-1])\n",
    "        dataY.append(df.iloc[i:(i + hoursteps),[0]])\n",
    "\n",
    "    print(\"len(dataX) : \", len(dataX), dataX[0].shape)\n",
    "    print(\"len(dataY) : \", len(dataY), dataY[0].shape)\n",
    "\n",
    "    #----------------------------------------------\n",
    "    #  Split train/test \n",
    "    #----------------------------------------------\n",
    "    train_size = int(len(dataX) * 0.7)\n",
    "    val_size   = int(len(dataX) * 0.2)\n",
    "    test_size  = len(dataX) - train_size - val_size\n",
    "    val_idx = train_size+val_size\n",
    "\n",
    "    trainX, valX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:val_idx]), np.array(dataX[val_idx:val_idx+test_size])\n",
    "    trainY, valY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:val_idx]), np.array(dataY[val_idx:val_idx+test_size])\n",
    "\n",
    "    print('train X : ', trainX.shape, '\\tY : ', trainY.shape)\n",
    "    print('val   X : ', valX.shape,   '\\tY : ', valY.shape)\n",
    "    print('test  X : ', testX.shape,  '\\tY : ', testY.shape)\n",
    "\n",
    "    np.save(\"npset/\"+season_mod+\"_trainX\",trainX)\n",
    "    np.save(\"npset/\"+season_mod+\"_trainY\",trainY)\n",
    "    np.save(\"npset/\"+season_mod+\"_valX\",valX)\n",
    "    np.save(\"npset/\"+season_mod+\"_valY\",valY)\n",
    "    np.save(\"npset/\"+season_mod+\"_testX\",testX)\n",
    "    np.save(\"npset/\"+season_mod+\"_testY\",testY)\n",
    "\n",
    "    #############################################\n",
    "    # Modeling\n",
    "    #############################################\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model.add(RepeatVector(hoursteps))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.summary()\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # model fit \n",
    "    #----------------------------------------------\n",
    "    for i in range(n_model):#0,5):#\n",
    "        #keras.optimizers.RMSprop(lr=0.005, rho=0.9, epsilon=None, decay=0.0)\n",
    "        model.compile(loss='mean_squared_error', optimizer=RMSProp())\n",
    "        hist = model.fit(trainX, trainY, epochs=300, batch_size=64, validation_data=(valX, valY))\n",
    "        results = model.evaluate(testX, testY)\n",
    "        #model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=0)\n",
    "        model.save('model/model_'+season_mod+'_'+str(i)+'.h5')# # of feature=3,5,7,9,?,12,14,16,18\n",
    "\n",
    "    #############################################\n",
    "    # train 과정 분석\n",
    "    #############################################\n",
    "    print('result : ', results)\n",
    "\n",
    "    end = 48\n",
    "    step = hoursteps*7\n",
    "    y = sc_pow.inverse_transform(trainY[:,:,0])\n",
    "    plotY = y.reshape(-1,1)\n",
    "    pred = model.predict(trainX)[:,:,0]\n",
    "    x = sc_pow.inverse_transform(pred)\n",
    "    plot_pred = x.reshape(-1,1)\n",
    "\n",
    "    acc_list = []\n",
    "    target_list=[]\n",
    "    for i in range(0, plotY.shape[0]-hoursteps, hoursteps):\n",
    "        pred = np.sum(plot_pred[i:i+hoursteps])\n",
    "        target      = round(np.sum(plotY[i:i+hoursteps]), 2)\n",
    "        \n",
    "        error       = round(np.abs(target-pred), 2)\n",
    "        error_rate  = round(error/target, 2)\n",
    "        acc_rate    = np.max([round((1.0-error_rate)*100, 2),0])\n",
    "        \n",
    "        target_list.append(target)\n",
    "        acc_list.append(acc_rate)\n",
    "\n",
    "    print(\"mean(acc rate): \",np.mean(acc_list),sep='')\n",
    "    print(acc_list)\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # model별 mean acc 출력\n",
    "    #----------------------------------------------\n",
    "    X_test = np.load(\"npset/\"+season_mod+\"_testX.npy\")\n",
    "    y_test = np.load(\"npset/\"+season_mod+\"_testY.npy\")\n",
    "\n",
    "    print(\"X_test : \", X_test.shape)\n",
    "    print(\"y_test : \", y_test.shape)\n",
    "\n",
    "    #############################################\n",
    "    # predict, 결과 분석\n",
    "    #############################################\n",
    "    n_dataset   = y_test.shape[0]\n",
    "    acc_list    = []\n",
    "    acc_model   = []\n",
    "    model       = []\n",
    "\n",
    "    for i in range(n_model):\n",
    "        model.append(load_model('model/model_'+season_mod+'_'+str(i)+'.h5'))\n",
    "        acc_model.append(0)\n",
    "        \n",
    "    print(\"[ dataset ]\")\n",
    "    for i in range(n_dataset):\n",
    "        y = sc_pow.inverse_transform(y_test[i:i+1,:,0])\n",
    "\n",
    "        for m in range(n_model):\n",
    "            pred = model[m].predict([X_test[i:i+1]])\n",
    "            pred[pred<0] = 0\n",
    "            pred = pred[:,:,0]\n",
    "            pred = sc_pow.inverse_transform(pred)\n",
    "            pred = np.sum(pred)\n",
    "\n",
    "            target      = round(np.sum(y), 2)\n",
    "            error       = round(np.abs(target-pred), 2)\n",
    "            error_rate  = np.min([round(error/target, 2),1])\n",
    "            acc_rate    = round((1.0-error_rate)*100, 2)\n",
    "            acc_list.append(acc_rate)\n",
    "            acc_model[m] += acc_rate\n",
    "            #print(\"   pred: \",pred,\" | target: \",target,\" | error: \",error,\" | err rate: \",error_rate,\" | acc: \",acc_rate,sep=\"\")\n",
    "            #print(\"acc rate: \",np.mean(acc_list[-n_model:]),sep='')\n",
    "        if(i%5==0): print(\" \")\n",
    "        print(np.mean(acc_list[-n_model:]), \" / \",sep='', end='')\n",
    "    print(\"mean(acc rate): \",np.mean(acc_list),sep='')\n",
    "    print(\"[ model ]\")\n",
    "    for i in range(n_model):\n",
    "        acc_model[i] = round(acc_model[i]/(n_dataset),2)\n",
    "        print(acc_model[i])\n",
    "\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
